[{"authors":["ruiming"],"categories":null,"content":"I am a Ph.D. student in Bioengineering at UC Berkeley, advised by Prof. Laura Waller. My current research is on the intersection of computational imaging, optics, and biomedical imaging. I am interested to retrieve the previously inaccessible information by the joint design of optical system hardware, physical modeling, and computing algorithms. I am also affiliated with the Berkeley Artificial Intelligence Research (BAIR). I was an research intern at Google Research in 2021 and an optical scientist intern in Meta Reality Labs Research in 2022\nBefore coming to Berkeley, I spent six years at UCLA and obtained B.S. in Computer Science and Applied Mathematics and M.S. in Computer Science. My past research was on computer vision and medical imaging. During M.S., I worked on MRI imaging analysis advised by Prof. Kyung Hyun Sung and Prof. Fabien Scalzo. In my undergrad, I worked on knowledge graph parsing/explainable AI advised by Prof. Song-Chun Zhu.\nBesides research, I spend time on downhill skiing, running, trekking, photography. And my name is pronounced as \u0026ldquo;Rei-Ming Tsao/Chao\u0026rdquo; :)\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"c7271ebcef3e1181bf08b35e3dcc6a4c","permalink":"https://rmcao.github.io/authors/ruiming/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ruiming/","section":"authors","summary":"I am a Ph.D. student in Bioengineering at UC Berkeley, advised by Prof. Laura Waller. My current research is on the intersection of computational imaging, optics, and biomedical imaging. I am interested to retrieve the previously inaccessible information by the joint design of optical system hardware, physical modeling, and computing algorithms.","tags":null,"title":"Ruiming Cao","type":"authors"},{"authors":["Ruiming Cao","Fanglin Linda Liu","Li-Hao Yeh","Laura Waller"],"categories":[],"content":"Structured illumination microscopy (SIM) reconstructs a super-resolved image from multiple raw images captured with different illumination patterns; hence, acquisition speed is limited, making it unsuitable for dynamic scenes. We propose a new method, Speckle Flow SIM, that uses static patterned illumination with moving samples and models the sample motion during data capture in order to reconstruct the dynamic scene with super-resolution. Speckle Flow SIM relies on sample motion to capture a sequence of raw images. The spatio-temporal relationship of the dynamic scene is modeled using a neural space-time model with coordinate-based multi-layer perceptrons (MLPs), and the motion dynamics and the super-resolved scene are jointly recovered. We validate Speckle Flow SIM for coherent imaging in simulation and build a simple, inexpensive experimental setup with off-the-shelf components. We demonstrate that Speckle Flow SIM can reconstruct a dynamic scene with deformable motion and 1.88x the diffraction-limited resolution in experiment.\n@article{cao2022dynamic, title={Dynamic Structured Illumination Microscopy with a Neural Space-time Model}, author={Cao, Ruiming and Liu, Fanglin Linda and Yeh, Li-Hao and Waller, Laura}, journal={arXiv preprint arXiv:2206.01397}, year={2022} }  ","date":1661442937,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661442937,"objectID":"9e995aa21cd97142c951c78153146f22","permalink":"https://rmcao.github.io/project/speckleflowsim/","publishdate":"2022-08-25T13:55:37-02:00","relpermalink":"/project/speckleflowsim/","section":"project","summary":"A new structured illumination microscopy (SIM) method, Speckle Flow SIM, that uses static patterned illumination on a dynamic scene and models the space-time relationship to super-resolve the dynamic scene.","tags":["CI"],"title":"Dynamic Structured Illumination Microscopy with a Neural Space-time Model","type":"project"},{"authors":["Ruiming Cao","Michael Kellman","David Ren","Regina Eckert","Laura Waller"],"categories":[],"content":"3D phase imaging recovers an object’s volumetric refractive index from intensity and/or holographic measurements. Partially coherent methods, such as illumination-based differential phase contrast (DPC), are particularly simple to implement in a commercial brightfield microscope. 3D DPC acquires images at multiple focus positions and with different illumination source patterns in order to reconstruct 3D refractive index. Here, we present a practical extension of the 3D DPC method that does not require a precise motion stage for scanning the focus and uses optimized illumination patterns for improved performance. The user scans the focus by hand, using the microscope’s focus knob, and the algorithm self-calibrates the axial position to solve for the 3D refractive index of the sample through a computational inverse problem. We further show that the illumination patterns can be optimized by an end-to-end learning procedure. Combining these two, we demonstrate improved 3D DPC with a commercial microscope whose only hardware modification is LED array illumination.\n","date":1645730977,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645730977,"objectID":"c818855ff79eea9c19f72142f63a7ed1","permalink":"https://rmcao.github.io/project/motion3ddpc/","publishdate":"2022-02-24T12:29:37-07:00","relpermalink":"/project/motion3ddpc/","section":"project","summary":"A practical extension of the 3D DPC method that does not require a precise motion stage for scanning the focus and uses optimized illumination patterns for improved 3D refractive index tomography.","tags":["CI"],"title":"Self-calibrated 3D differential phase contrast microscopy with optimized illumination","type":"project"},{"authors":["Ruiming Cao","Xinran Zhong","Sohrab Afshari","Ely Felker","Voraparee Suvannarerg","Teeravut Tubtawee","Sitaram Vangala","Fabien Scalzo","Steven Raman","Kyunghyun Sung"],"categories":[],"content":"Background: Several deep learning-based techniques have been developed for prostate cancer (PCa) detection using multi-parametric MRI (mpMRI), but few of them have been rigorously evaluated relative to radiologists’ performance or whole-mount histopathology (WMHP). Purpose: To compare the performance of a previously proposed deep learning algorithm, FocalNet, and expert radiologists in the detection of PCa on mpMRI with WMHP as the reference. Study type: Retrospective, single-center study. Subjects: 553 patients (development cohort: 427 patients; evaluation cohort: 126 patients) who underwent 3 T mpMRI prior to radical prostatectomy from October 2010 to February 2018. Field Strength/Sequence: 3 T, T2-weighted imaging and diffusion-weighted imaging. Assessment: FocalNet was trained on the development cohort with the groundtruth lesion annotations. In evaluation cohort, FocalNet predicted PCa locations by detection points, with a confidence value for each point. Four fellowship-trained genitourinary (GU) radiologists independently evaluated the evaluation cohort to detect suspicious PCa foci, annotate detection point locations, and assign a five-point suspicion score (suspicion score 1: the least suspicious to PCa, suspicion score 5: the most suspicious to PCa) for each annotated detection point. The PCa detection performance of FocalNet and radiologists were evaluated by free-response receiver operating characteristics analysis for lesion detection sensitivity versus the number of false-positive detections at different thresholds on suspicion scores. The overall differential detection sensitivity is the detection sensitivity difference between each radiologist and FocalNet averaging over five suspicion score thresholds and four readers. Index lesions are defined as lesions with the highest Gleason Group and the largest pathological size, and clinically significant lesions are those with Gleason Group ≥ 2 or pathological size ≥10 mm. Statistical tests: Bootstrap hypothesis test for the detection sensitivity between radiologists and FocalNet. Results: For detection points under high with suspicion score 5, index lesion detection sensitivity was 38.3% for the radiologists and 38.7% for FocalNet at the cost of 0.101 false positives per patient. For detection points with suspicion score 4 or 5, index lesion detection sensitivity was 63.1% for the radiologists and 54.7% for FocalNet at the cost of 0.365 false positives per patient. For the overall differential detection sensitivity, FocalNet was 5.1% and 4.7% below the radiologists for clinically significant and index lesions, respectively; however, the differences were not statistically significant (P=0.413 and P=0.282, respectively). Data Conclusion: On the evaluation cohort, FocalNet achieved slightly lower but not statistically significant PCa detection performance compared to GU radiologists. Compared with radiologists, FocalNet demonstrated similar detection performance for a highly sensitive setting (suspicion score ≥ 1) or a highly specific setting (suspicion score = 5) while lower performance in between.\n","date":1615497155,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615497155,"objectID":"e11deed2fe2381b5b5723f3dfa344aa2","permalink":"https://rmcao.github.io/project/radaicomparison/","publishdate":"2021-03-11T14:12:35-07:00","relpermalink":"/project/radaicomparison/","section":"project","summary":"A validation study of our previously developed algorithm for prostate cancer detection using an independent patient cohort who went through radical prostatectomy and compared with genitourinary radiologists under the same setting.","tags":["BI"],"title":"GU Radiologists vs AI in Detection of Prostate Cancer on MRI","type":"project"},{"authors":["Ruiming Cao","Amir Bajgiran","Sohrab Afshari","Sepideh Shakeri","Xinran Zhong","Dieter Enzmann","Steven Raman","Kyunghyun Sung"],"categories":[],"content":"Multi-parametric MRI (mp-MRI) is considered the best non-invasive imaging modality for diagnosing prostate cancer (PCa). However, mp-MRI for PCa diagnosis is currently limited by the qualitative or semi-quantitative interpretation criteria, leading to inter-reader variability and a suboptimal ability to assess lesion aggressiveness. Convolutional neural networks (CNNs) are a powerful method to automatically learn the discriminative features for various tasks, including cancer detection. We propose a novel multi-class CNN, FocalNet, to jointly detect PCa lesions and predict their aggressiveness using Gleason score (GS). FocalNet characterizes lesion aggressiveness and fully utilizes distinctive knowledge from mp-MRI. We collected a prostate mp-MRI dataset from 417 patients who underwent 3T mp-MRI exams prior to robotic-assisted laparoscopic prostatectomy (RALP). FocalNet is trained and evaluated in this large study cohort with 5-fold cross-validation. In the free-response receiver operating characteristics (FROC) analysis for lesion detection, FocalNet achieved 89.7% and 87.9% sensitivity for index lesions and clinically significant lesions at 1 false positive per patient, respectively. For GS classification, evaluated by the receiver operating characteristics (ROC) analysis, FocalNet received the area under the curve (AUC) of 0.81 and 0.79 for the classifications of clinically significant PCa (GS\u0026gt;=3+4) and PCa with GS\u0026gt;=4+3, respectively. With the comparison to the prospective performance of radiologists using the current diagnostic guideline, FocalNet demonstrated comparable detection sensitivity for index lesions and clinically significant lesions, only 3.4% and 1.5% lower than highly experienced radiologists without statistical significance.\n","date":1551293613,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551293613,"objectID":"3afbfa784af4969164d1c461c8f43f0e","permalink":"https://rmcao.github.io/project/focalnet/","publishdate":"2019-02-27T11:53:33-07:00","relpermalink":"/project/focalnet/","section":"project","summary":"We proposed a multi-class CNN to jointly detect prostate cancer lesions and characterizes their histopathological aggressiveness by fully utilizing distinctive knowledge from multi-parametric MRI.","tags":["BI"],"title":"Joint Prostate Cancer Detection and Histological Score Prediction on MRI","type":"project"},{"authors":["Quanshi Zhang","Ruiming Cao","Feng Shi","Ying Nian Wu","Song-Chun Zhu"],"categories":[],"content":"","date":1529520888,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529520888,"objectID":"d5545455cab64090b85cd28857c27e3e","permalink":"https://rmcao.github.io/project/explanatorygraph/","publishdate":"2018-06-20T11:54:48-07:00","relpermalink":"/project/explanatorygraph/","section":"project","summary":"We proposed to automatically discover object parts in an unsupervised manner, which disentangles feature components of object parts from feature representations of each convolutional filter.","tags":["CV"],"title":"Interpreting CNN Knowledge via an Explanatory Graph","type":"project"},{"authors":["Quanshi Zhang","Ruiming Cao","Ying Nian Wu","Song-Chun Zhu"],"categories":[],"content":"Given a convolutional neural network (CNN) that is pre-trained for object classification, this paper proposes to use active question-answering to semanticize neural patterns in conv-layers of the CNN and mine part concepts. For each part concept, we mine neural patterns in the pre-trained CNN, which are related to the target part, and use these patterns to construct an And-Or graph (AOG) to represent a four-layer semantic hierarchy of the part. As an interpretable model, the AOG associates different CNN units with different explicit object parts. We use an active human-computer communication to incrementally grow such an AOG on the pre-trained CNN as follows. We allow the computer to actively identify objects, whose neural patterns cannot be explained by the current AOG. Then, the computer asks human about the unexplained objects, and uses the answers to automatically discover certain CNN patterns corresponding to the missing knowledge. We incrementally grow the AOG to encode new knowledge discovered during the active-learning process. In experiments, our method exhibits high learning efficiency. Our method uses about 1/6-1/3 of the part annotations for training, but achieves similar or better part-localization performance than fast-RCNN methods.\n","date":1513798676,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513798676,"objectID":"4e8068f1c3e69caf1afcb90e046c5e02","permalink":"https://rmcao.github.io/project/growingpartgraph/","publishdate":"2017-12-20T12:37:56-07:00","relpermalink":"/project/growingpartgraph/","section":"project","summary":"We used active question-answering to weakly-supervised semanticize neural patterns in conv-layers of the CNN and mine part concepts.","tags":["CV"],"title":"Mining Object Parts from CNNs via Active Question-Answering","type":"project"}]