<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ruiming&#39;s website</title>
    <link>https://rmcao.github.io/</link>
      <atom:link href="https://rmcao.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Ruiming&#39;s website</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2022 Ruiming Cao | Website icon from openmoji.org</copyright><lastBuildDate>Thu, 24 Feb 2022 12:29:37 -0700</lastBuildDate>
    <image>
      <url>https://rmcao.github.io/images/icon_hucd15fd8e38fe1904d624914b135f094f_55420_512x512_fill_lanczos_center_2.png</url>
      <title>Ruiming&#39;s website</title>
      <link>https://rmcao.github.io/</link>
    </image>
    
    <item>
      <title>Self-calibrated 3D differential phase contrast microscopy with optimized illumination</title>
      <link>https://rmcao.github.io/project/motion3ddpc/</link>
      <pubDate>Thu, 24 Feb 2022 12:29:37 -0700</pubDate>
      <guid>https://rmcao.github.io/project/motion3ddpc/</guid>
      <description>&lt;p&gt;3D phase imaging recovers an object’s volumetric refractive index from intensity and/or holographic measurements. Partially coherent methods, such as illumination-based differential phase contrast (DPC), are particularly simple to implement in a commercial brightfield microscope. 3D DPC acquires images at multiple focus positions and with different illumination source patterns in order to reconstruct 3D refractive index. Here, we present a practical extension of the 3D DPC method that does not require a precise motion stage for scanning the focus and uses optimized illumination patterns for improved performance. The user scans the focus by hand, using the microscope’s focus knob, and the algorithm self-calibrates the axial position to solve for the 3D refractive index of the sample through a computational inverse problem. We further show that the illumination patterns can be optimized by an end-to-end learning procedure. Combining these two, we demonstrate improved 3D DPC with a commercial microscope whose only hardware modification is LED array illumination.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GU Radiologists vs AI in Detection of Prostate Cancer on MRI</title>
      <link>https://rmcao.github.io/project/radaicomparison/</link>
      <pubDate>Thu, 11 Mar 2021 14:12:35 -0700</pubDate>
      <guid>https://rmcao.github.io/project/radaicomparison/</guid>
      <description>&lt;p&gt;Background: Several deep learning-based techniques have been developed for prostate cancer (PCa) detection using multi-parametric MRI (mpMRI), but few of them have been rigorously evaluated relative to radiologists’ performance or whole-mount histopathology (WMHP).
Purpose: To compare the performance of a previously proposed deep learning algorithm, FocalNet, and expert radiologists in the detection of PCa on mpMRI with WMHP as the reference.
Study type: Retrospective, single-center study.
Subjects: 553 patients (development cohort: 427 patients; evaluation cohort: 126 patients) who underwent 3 T mpMRI prior to radical prostatectomy from October 2010 to February 2018.
Field Strength/Sequence: 3 T, T2-weighted imaging and diffusion-weighted imaging.
Assessment: FocalNet was trained on the development cohort with the groundtruth lesion annotations. In evaluation cohort, FocalNet predicted PCa locations by detection points, with a confidence value for each point. Four fellowship-trained genitourinary (GU) radiologists independently evaluated the evaluation cohort to detect suspicious PCa foci, annotate detection point locations, and assign a five-point suspicion score (suspicion score 1: the least suspicious to PCa, suspicion score 5: the most suspicious to PCa) for each annotated detection point. The PCa detection performance of FocalNet and radiologists were evaluated by free-response receiver operating characteristics analysis for lesion detection sensitivity versus the number of false-positive detections at different thresholds on suspicion scores. The overall differential detection sensitivity is the detection sensitivity difference between each radiologist and FocalNet averaging over five suspicion score thresholds and four readers. Index lesions are defined as lesions with the highest Gleason Group and the largest pathological size, and clinically significant lesions are those with Gleason Group ≥ 2 or pathological size ≥10 mm.
Statistical tests:  Bootstrap hypothesis test for the detection sensitivity between radiologists and FocalNet.
Results: For detection points under high  with suspicion score 5, index lesion detection sensitivity was 38.3% for the radiologists and 38.7% for FocalNet at the cost of 0.101 false positives per patient. For detection points with suspicion score 4 or 5, index lesion detection sensitivity was 63.1% for the radiologists and 54.7% for FocalNet at the cost of 0.365 false positives per patient. For the overall differential detection sensitivity, FocalNet was 5.1% and 4.7% below the radiologists for clinically significant and index lesions, respectively; however, the differences were not statistically significant (P=0.413 and P=0.282, respectively).
Data Conclusion: On the evaluation cohort, FocalNet achieved slightly lower but not statistically significant PCa detection performance compared to GU radiologists. Compared with radiologists, FocalNet demonstrated similar detection performance for a highly sensitive setting (suspicion score ≥ 1) or a highly specific setting (suspicion score = 5) while lower performance in between.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Joint Prostate Cancer Detection and Histological Score Prediction on MRI</title>
      <link>https://rmcao.github.io/project/focalnet/</link>
      <pubDate>Wed, 27 Feb 2019 11:53:33 -0700</pubDate>
      <guid>https://rmcao.github.io/project/focalnet/</guid>
      <description>&lt;p&gt;Multi-parametric MRI (mp-MRI) is considered the best non-invasive imaging modality for diagnosing prostate cancer (PCa). However, mp-MRI for PCa diagnosis is currently limited by the qualitative or semi-quantitative interpretation criteria, leading to inter-reader variability and a suboptimal ability to assess lesion aggressiveness. Convolutional neural networks (CNNs) are a powerful method to automatically learn the discriminative features for various tasks, including cancer detection. We propose a novel multi-class CNN, FocalNet, to jointly detect PCa lesions and predict their aggressiveness using Gleason score (GS). FocalNet characterizes lesion aggressiveness and fully utilizes distinctive knowledge from mp-MRI. We collected a prostate mp-MRI dataset from 417 patients who underwent 3T mp-MRI exams prior to robotic-assisted laparoscopic prostatectomy (RALP). FocalNet is trained and evaluated in this large study cohort with 5-fold cross-validation. In the free-response receiver operating characteristics (FROC) analysis for lesion detection, FocalNet achieved 89.7% and 87.9% sensitivity for index lesions and clinically significant lesions at 1 false positive per patient, respectively. For GS classification, evaluated by the receiver operating characteristics (ROC) analysis, FocalNet received the area under the curve (AUC) of 0.81 and 0.79 for the classifications of clinically significant PCa (GS&amp;gt;=3+4) and PCa with GS&amp;gt;=4+3, respectively. With the comparison to the prospective performance of radiologists using the current diagnostic guideline, FocalNet demonstrated comparable detection sensitivity for index lesions and clinically significant lesions, only 3.4% and 1.5% lower than highly experienced radiologists without statistical significance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interpreting CNN Knowledge via an Explanatory Graph</title>
      <link>https://rmcao.github.io/project/explanatorygraph/</link>
      <pubDate>Wed, 20 Jun 2018 11:54:48 -0700</pubDate>
      <guid>https://rmcao.github.io/project/explanatorygraph/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mining Object Parts from CNNs via Active Question-Answering</title>
      <link>https://rmcao.github.io/project/growingpartgraph/</link>
      <pubDate>Wed, 20 Dec 2017 12:37:56 -0700</pubDate>
      <guid>https://rmcao.github.io/project/growingpartgraph/</guid>
      <description>&lt;p&gt;Given a convolutional neural network (CNN) that is pre-trained for object classification, this paper proposes to use active question-answering to semanticize neural patterns in conv-layers of the CNN and mine part concepts. For each part concept, we mine neural patterns in the pre-trained CNN, which are related to the target part, and use these patterns to construct an And-Or graph (AOG) to represent a four-layer semantic hierarchy of the part. As an interpretable model, the AOG associates different CNN units with different explicit object parts. We use an active human-computer communication to incrementally grow such an AOG on the pre-trained CNN as follows. We allow the computer to actively identify objects, whose neural patterns cannot be explained by the current AOG. Then, the computer asks human about the unexplained objects, and uses the answers to automatically discover certain CNN patterns corresponding to the missing knowledge. We incrementally grow the AOG to encode new knowledge discovered during the active-learning process. In experiments, our method exhibits high learning efficiency. Our method uses about 1/6-1/3 of the part annotations for training, but achieves similar or better part-localization performance than fast-RCNN methods.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
