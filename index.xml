<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ruiming&#39;s website</title>
    <link>https://rmcao.github.io/</link>
      <atom:link href="https://rmcao.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Ruiming&#39;s website</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2020 Ruiming Cao | Website icon from openmoji.org</copyright><lastBuildDate>Fri, 26 Jun 2020 12:29:37 -0700</lastBuildDate>
    <image>
      <url>https://rmcao.github.io/images/icon_hucd15fd8e38fe1904d624914b135f094f_55420_512x512_fill_lanczos_center_2.png</url>
      <title>Ruiming&#39;s website</title>
      <link>https://rmcao.github.io/</link>
    </image>
    
    <item>
      <title>Algorithmic self-calibration for 3D differential phase contrast microscopy</title>
      <link>https://rmcao.github.io/project/motion3ddpc/</link>
      <pubDate>Fri, 26 Jun 2020 12:29:37 -0700</pubDate>
      <guid>https://rmcao.github.io/project/motion3ddpc/</guid>
      <description></description>
    </item>
    
    <item>
      <title>GU Radiologists vs AI in Detection of Prostate Cancer on MRI</title>
      <link>https://rmcao.github.io/project/radaicomparison/</link>
      <pubDate>Thu, 20 Feb 2020 14:12:35 -0700</pubDate>
      <guid>https://rmcao.github.io/project/radaicomparison/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Joint Prostate Cancer Detection and Histological Score Prediction on MRI</title>
      <link>https://rmcao.github.io/project/focalnet/</link>
      <pubDate>Wed, 27 Feb 2019 11:53:33 -0700</pubDate>
      <guid>https://rmcao.github.io/project/focalnet/</guid>
      <description>&lt;p&gt;Multi-parametric MRI (mp-MRI) is considered the best non-invasive imaging modality for diagnosing prostate cancer (PCa). However, mp-MRI for PCa diagnosis is currently limited by the qualitative or semi-quantitative interpretation criteria, leading to inter-reader variability and a suboptimal ability to assess lesion aggressiveness. Convolutional neural networks (CNNs) are a powerful method to automatically learn the discriminative features for various tasks, including cancer detection. We propose a novel multi-class CNN, FocalNet, to jointly detect PCa lesions and predict their aggressiveness using Gleason score (GS). FocalNet characterizes lesion aggressiveness and fully utilizes distinctive knowledge from mp-MRI. We collected a prostate mp-MRI dataset from 417 patients who underwent 3T mp-MRI exams prior to robotic-assisted laparoscopic prostatectomy (RALP). FocalNet is trained and evaluated in this large study cohort with 5-fold cross-validation. In the free-response receiver operating characteristics (FROC) analysis for lesion detection, FocalNet achieved 89.7% and 87.9% sensitivity for index lesions and clinically significant lesions at 1 false positive per patient, respectively. For GS classification, evaluated by the receiver operating characteristics (ROC) analysis, FocalNet received the area under the curve (AUC) of 0.81 and 0.79 for the classifications of clinically significant PCa (GS&amp;gt;=3+4) and PCa with GS&amp;gt;=4+3, respectively. With the comparison to the prospective performance of radiologists using the current diagnostic guideline, FocalNet demonstrated comparable detection sensitivity for index lesions and clinically significant lesions, only 3.4% and 1.5% lower than highly experienced radiologists without statistical significance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Interpreting CNN Knowledge via an Explanatory Graph</title>
      <link>https://rmcao.github.io/project/explanatorygraph/</link>
      <pubDate>Wed, 20 Jun 2018 11:54:48 -0700</pubDate>
      <guid>https://rmcao.github.io/project/explanatorygraph/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mining Object Parts from CNNs via Active Question-Answering</title>
      <link>https://rmcao.github.io/project/growingpartgraph/</link>
      <pubDate>Wed, 20 Dec 2017 12:37:56 -0700</pubDate>
      <guid>https://rmcao.github.io/project/growingpartgraph/</guid>
      <description>&lt;p&gt;Given a convolutional neural network (CNN) that is pre-trained for object classification, this paper proposes to use active question-answering to semanticize neural patterns in conv-layers of the CNN and mine part concepts. For each part concept, we mine neural patterns in the pre-trained CNN, which are related to the target part, and use these patterns to construct an And-Or graph (AOG) to represent a four-layer semantic hierarchy of the part. As an interpretable model, the AOG associates different CNN units with different explicit object parts. We use an active human-computer communication to incrementally grow such an AOG on the pre-trained CNN as follows. We allow the computer to actively identify objects, whose neural patterns cannot be explained by the current AOG. Then, the computer asks human about the unexplained objects, and uses the answers to automatically discover certain CNN patterns corresponding to the missing knowledge. We incrementally grow the AOG to encode new knowledge discovered during the active-learning process. In experiments, our method exhibits high learning efficiency. Our method uses about 1/6-1/3 of the part annotations for training, but achieves similar or better part-localization performance than fast-RCNN methods.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
